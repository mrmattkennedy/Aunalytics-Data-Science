{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First row is headers, so just simple import on the csv data using pandas\n",
    "train_csv = pd.read_csv(\"au_train.csv\")\n",
    "test_csv = pd.read_csv(\"au_test.csv\")\n",
    "\n",
    "#Remove \"education\" column from each, as the next column is a numerical representation of it\n",
    "train_csv.drop('education', axis=1, inplace=True)\n",
    "test_csv.drop('education', axis=1, inplace=True)\n",
    "\n",
    "#Remove period from last character in class for test cases\n",
    "test_csv['class'] = test_csv['class'].str.replace('.', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the data, there are 14 variable columns, as well as the \"class\", or target column.\n",
    "We want to convert object  columns to discrete data. We can do this by hand, but an easier way is to use pandas builtin Categorical functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert object columns to discrete numerical values\n",
    "for col in train_csv:\n",
    "    if train_csv[col].dtype == np.dtype('object'):\n",
    "        temp_col = pd.Categorical(train_csv[col])\n",
    "        temp_col = temp_col.codes\n",
    "        train_csv[col] = temp_col\n",
    "\n",
    "#Do the same for the test data\n",
    "for col in test_csv:\n",
    "    if test_csv[col].dtype == np.dtype('object'):\n",
    "        temp_col = pd.Categorical(test_csv[col])\n",
    "        temp_col = temp_col.codes\n",
    "        test_csv[col] = temp_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data for tensorflow and start making the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pop the class columns off each dataset to save as targets for each\n",
    "train_y = train_csv.pop('class')\n",
    "test_y = test_csv.pop('class')\n",
    "\n",
    "#Load train csv into tensor, then shuffle and create batches\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_csv.values, train_y.values))\n",
    "train_dataset = train_dataset.shuffle(len(train_csv)).batch(50)\n",
    "\n",
    "#Load test csv into tensor - need to create batch for evaluation function to work later\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_csv.values, test_y.values))\n",
    "test_dataset = test_dataset.batch(len(test_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    tf.keras.layers.Dense(250, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "652/652 [==============================] - 1s 1ms/step - loss: 862.3832 - accuracy: 0.6773\n",
      "Epoch 2/5\n",
      "652/652 [==============================] - 1s 1ms/step - loss: 276.0750 - accuracy: 0.6869\n",
      "Epoch 3/5\n",
      "652/652 [==============================] - 1s 1ms/step - loss: 141.7698 - accuracy: 0.6819\n",
      "Epoch 4/5\n",
      "652/652 [==============================] - 1s 1ms/step - loss: 73.0040 - accuracy: 0.6860\n",
      "Epoch 5/5\n",
      "652/652 [==============================] - 1s 1ms/step - loss: 45.0917 - accuracy: 0.6858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c5c8450c10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit(train_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1000us/step - loss: 19.4957 - accuracy: 0.7966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19.49567413330078, 0.7966341376304626]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on naive bayes, why naive bayes, start by getting the 3 categories (probabilities of each class, each var, each var for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset datasets\n",
    "#First row is headers, so just simple import on the csv data using pandas\n",
    "train_csv = pd.read_csv(\"au_train.csv\")\n",
    "test_csv = pd.read_csv(\"au_test.csv\")\n",
    "\n",
    "#Remove \"education\" column from each, as the next column is a numerical representation of it\n",
    "train_csv.drop('education', axis=1, inplace=True)\n",
    "test_csv.drop('education', axis=1, inplace=True)\n",
    "\n",
    "#Remove period from last character in class for test cases\n",
    "test_csv['class'] = test_csv['class'].str.replace('.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_probability(mean, var, val):\n",
    "    return 1/(math.sqrt(2*math.pi)*var) * math.e**-((val-mean)/var**2)\n",
    "\n",
    "def get_dataset_probabilities(ds):\n",
    "    #Get # of rows in dataset\n",
    "    total = len(ds)\n",
    "    feature_probs = {}\n",
    "    \n",
    "    for col in ds:\n",
    "        #Don't want to count target col\n",
    "        if col == 'class':\n",
    "            continue\n",
    "\n",
    "        #If column is continuous, use Gaussian Distribution to get probability\n",
    "        if ds[col].dtype == np.dtype('int64'):\n",
    "            feature_probs[col] = {'mean': np.mean(ds[col]), 'var': np.var(ds[col])}#get_continuous_probability(ds[col])\n",
    "\n",
    "        #If column is object, count each item and divide by # of rows\n",
    "        elif ds[col].dtype == np.dtype('object'):\n",
    "            temp = ds[col].value_counts().to_dict()\n",
    "            for i in temp.keys():\n",
    "                temp[i] = temp[i]/total\n",
    "\n",
    "            feature_probs[col] = temp\n",
    "    \n",
    "    return feature_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get class counts: get the total counts of each class, then divide by # of rows\n",
    "total = len(train_csv)\n",
    "class_probs = train_csv['class'].value_counts().to_dict()\n",
    "for i in class_probs.keys():\n",
    "    class_probs[i] = class_probs[i]/total\n",
    "\n",
    "#Feature counts\n",
    "feature_probs = get_dataset_probabilities(train_csv)\n",
    "#Features based on class\n",
    "feature_class_probs = {}\n",
    "for k in class_probs.keys():\n",
    "    #Just get the rows belonging to the specific class\n",
    "    current_class = train_csv.loc[train_csv['class'] == k]\n",
    "    feature_class_probs[k] = get_dataset_probabilities(current_class)\n",
    "    \n",
    "#Get list of keys for iteration\n",
    "key_list = list(feature_probs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have probabilities, let's iterate over each row and guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataset(ds):\n",
    "    total_right = 0\n",
    "    for i, row in ds.iterrows():\n",
    "        \n",
    "        #Iterate over each class to see which class has greatest probability\n",
    "        most_likely = defaultdict(float)\n",
    "        for cls in list(class_probs.keys()):\n",
    "            \n",
    "            #Reset likelihood and prior to 1\n",
    "            likelihood = 1\n",
    "            prior = 1\n",
    "\n",
    "            #Iterate over each column in row\n",
    "            for col_idx, item in enumerate(row):\n",
    "                #If item is the target class, ignore\n",
    "                if item in list(class_probs.keys()):\n",
    "                    continue\n",
    "                \n",
    "                #If int (continuous), get the probability with mean and variance\n",
    "                if type(item) == int:\n",
    "                    #Likelihood\n",
    "                    mean = feature_class_probs[cls][key_list[col_idx]]['mean']\n",
    "                    var = feature_class_probs[cls][key_list[col_idx]]['var']\n",
    "                    likelihood *= get_continuous_probability(mean, var, item)\n",
    "\n",
    "                    #Prior\n",
    "                    mean = feature_probs[key_list[col_idx]]['mean']\n",
    "                    var = feature_probs[key_list[col_idx]]['var']\n",
    "                    prior *= get_continuous_probability(mean, var, item)\n",
    "\n",
    "                #If str (object), get probability that was saved earlier\n",
    "                elif type(item) == str:\n",
    "                    #Key doesn't exist for that class\n",
    "                    try:\n",
    "                        likelihood *= feature_class_probs[cls][key_list[col_idx]][item]\n",
    "                    except KeyError:\n",
    "                        likelihood *= 0   \n",
    "                    prior = feature_probs[key_list[col_idx]][item]\n",
    "\n",
    "            most_likely[cls] = (likelihood * class_probs[cls]) / prior\n",
    "\n",
    "        selection = max(most_likely, key=most_likely.get)\n",
    "        if selection == row[-1]:\n",
    "            total_right+=1\n",
    "    \n",
    "    return total_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7591904425539756\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset(train_csv) / len(train_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7637737239727289\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset(test_csv) / len(test_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis on results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
